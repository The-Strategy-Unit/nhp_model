{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic datasets\n",
    "\n",
    "The following notebook will generate a simple synthetic dataset for testing the model with.\n",
    "\n",
    "**This is not a truely synthetic dataset** as we sample from **patient identifiable** data. It *must* be treated as PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/dev\"\n",
    "fyear = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(path):\n",
    "    p = f\"{path}/{i}/fyear={fyear}/dataset=synthetic\"\n",
    "    if os.path.exists(p):\n"
    "        shutil.rmtree(p)\n",
    "    os.makedirs(p, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "\n",
    "p = f\"{path}/ip/fyear={fyear}\"\n",
    "ip = (\n",
    "    pd.read_parquet(p)\n",
    "    .sample(n=N)\n",
    "    .drop(columns=\"dataset\")\n",
    "    .assign(sitetret=np.random.choice([\"a\", \"b\", \"c\"], N))\n",
    ")\n",
    "\n",
    "ip.to_parquet(f\"{p}/dataset=synthetic/0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [\"activity_avoidance\", \"efficiencies\"]:\n",
    "    p = f\"{path}/ip_{s}_strategies/fyear={fyear}\"\n",
    "    ip_s = pd.read_parquet(p).merge(ip[[\"rn\"]], on=\"rn\").drop(columns=\"dataset\")\n",
    "    ip_s.to_parquet(f\"{p}/dataset=synthetic/0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = f\"{path}/aae/fyear={fyear}\"\n",
    "aae = pd.read_parquet(p).drop(columns=[\"index\", \"dataset\"]).assign(sitetret=\"a\")\n",
    "\n",
    "aae = (\n",
    "    aae.groupby(list(aae.drop(columns=\"arrivals\").columns), as_index=False)[\"arrivals\"]\n",
    "    .sum()\n",
    "    .assign(arrivals=lambda r: np.random.poisson(r[\"arrivals\"] / len(os.listdir(p))))\n",
    "    .query(\"arrivals > 0\")\n",
    ")\n",
    "\n",
    "aae[\"rn\"] = [str(uuid.uuid4()) for _ in aae.index]\n",
    "\n",
    "aae.to_parquet(f\"{p}/dataset=synthetic/0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = f\"{path}/op/fyear={fyear}\"\n",
    "op = pd.read_parquet(p).drop(columns=[\"index\", \"dataset\"]).assign(sitetret=\"a\")\n",
    "\n",
    "op = (\n",
    "    op.groupby(\n",
    "        list(op.drop(columns=[\"attendances\", \"tele_attendances\"]).columns),\n",
    "        as_index=False,\n",
    "    )[[\"attendances\", \"tele_attendances\"]]\n",
    "    .sum()\n",
    "    .assign(\n",
    "        attendances=lambda r: np.random.poisson(r[\"attendances\"] / len(os.listdir(p))),\n",
    "        tele_attendances=lambda r: np.random.poisson(\n",
    "            r[\"tele_attendances\"] / len(os.listdir(p))\n",
    "        ),\n",
    "    )\n",
    "    .query(\"(attendances > 0) or (tele_attendances > 0)\")\n",
    ")\n",
    "\n",
    "op[\"rn\"] = [str(uuid.uuid4()) for _ in op.index]\n",
    "\n",
    "\n",
    "op.to_parquet(f\"{p}/dataset=synthetic/0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = f\"{path}/birth_factors/fyear={fyear}\"\n",
    "birth_factors = (\n",
    "    pd.read_parquet(p)\n",
    "    .drop(columns=\"dataset\")\n",
    "    .groupby([\"variant\", \"sex\", \"age\"], as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "birth_factors.to_parquet(f\"{p}/dataset=synthetic/0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = f\"{path}/demographic_factors/fyear={fyear}\"\n",
    "demographic_factors = (\n",
    "    pd.read_parquet(p)\n",
    "    .drop(columns=\"dataset\")\n",
    "    .groupby([\"variant\", \"sex\", \"age\"], as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "demographic_factors.to_parquet(f\"{p}/dataset=synthetic/0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = f\"{path}/hsa_activity_tables/fyear={fyear}\"\n",
    "hsa_activity_tables = (\n",
    "    pd.read_parquet(p)\n",
    "    .drop(columns=\"dataset\")\n",
    "    .groupby([\"hsagrp\", \"sex\", \"age\"], as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "hsa_activity_tables.to_parquet(f\"{p}/dataset=synthetic/0.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nhp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
