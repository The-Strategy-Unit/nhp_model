{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Hospitals Model\n",
    "\n",
    "This notebook runs the NHP model and produces the raw results.\n",
    "\n",
    "Note, this can take a very long time to run and load the resulting data. If you find that you are running out of RAM (especially when loading data) consider reducing the number of model runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = \"sample_params.json\"\n",
    "data_path = \"data\"\n",
    "results_path = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from run_model import run_model\n",
    "\n",
    "from model.aae import AaEModel\n",
    "from model.inpatients import InpatientsModel\n",
    "from model.outpatients import OutpatientsModel\n",
    "from model.model_save import LocalSave\n",
    "from model.helpers import load_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load in the params json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_params(params_file)\n",
    "# extract the number of model_runs the params calls for\n",
    "model_runs = params[\"model_runs\"]\n",
    "# set the create_datetime\n",
    "params[\"create_datetime\"] = f\"{datetime.now():%Y%m%d_%H%M%S}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the model in parallel. By default, use all available CPU cores. You can set this to a lower value to use less resources, but it will take longer to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = os.cpu_count()\n",
    "cpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the model in parallel it's slightly more efficient to run a batch of model runs. Batches of 4 or 8 seems to be most efficient. This value should be a power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 ** 2\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model run's it will create separate files for each model run - we store these in a temporary location before combining later. This creates a unique path to store the model results in which can easily be deleted later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(results_path, str(uuid.uuid4()))\n",
    "results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the model runner. The `run_model()` function expects the params dictionary, the path to the data, the path where the results will be saved, which model run to start at, how many model runs to perform, the number of CPU cores to use, and the size of the batches to run.\n",
    "\n",
    "The function returns a function, which takes either `AaEModel`, `InpatientsModel`, or `OutpatientsModel`, depending on what type of model we want to run.\n",
    "\n",
    "Note, we add one to the model runs. The \"principal\" model run is model run 0, and then we perform 1 to `model_runs` iterations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = run_model(\n",
    "    params,\n",
    "    data_path,\n",
    "    LocalSave,\n",
    "    results_path,\n",
    "    0,\n",
    "    model_runs + 1,\n",
    "    cpus,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the runner is set up, we can run each of the types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner(AaEModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner(OutpatientsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner(InpatientsModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load in our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(activity_type):\n",
    "  p = f\"{results_path}/model_results/activity_type={activity_type}/\"\n",
    "  return (ds\n",
    "    .dataset(p)\n",
    "    .to_table()\n",
    "    .to_pandas()\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = load_dataset(\"aae\").drop([\"rn\"], axis = \"columns\")\n",
    "aae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ip data needs to be handled slightly differently: we need to split out the op rows and add them back to the op dataset. we also need to join back to the baseline data to get the additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ip_op_data():\n",
    "  op = load_dataset(\"op\").drop([\"rn\", \"is_surgical_specialty\",\t\"is_adult\", \"type\"], axis = \"columns\")\n",
    "  ip = load_dataset(\"ip\")\n",
    "  ip_op_rows = ip[\"classpat\"] == \"-1\"\n",
    "\n",
    "  op_rows = (ip[ip_op_rows]\n",
    "    .groupby([\"age\", \"sex\", \"tretspef\", \"dataset\", \"scenario\", \"create_datetime\", \"model_run\"], as_index = False)\n",
    "    .agg({\"rn\": len})\n",
    "    .rename(columns = {\"rn\": \"attendances\"})\n",
    "    .assign(is_gp_ref = False, is_cons_cons_ref = False, is_first = False, has_procedures = True, tele_attendances = 0)\n",
    "  )\n",
    "  op_rows\n",
    "\n",
    "  op_fixed = pd.concat([op, op_rows]).groupby([\n",
    "    \"age\", \"sex\", \"tretspef\", \"is_gp_ref\", \"is_cons_cons_ref\", \"is_first\",\n",
    "    \"has_procedures\", \"dataset\", \"scenario\", \"create_datetime\", \"model_run\"],\n",
    "    as_index=False\n",
    "  ).agg({\"attendances\": np.sum, \"tele_attendances\": np.sum})\n",
    "\n",
    "  ip_rows = ip[~ip_op_rows]\n",
    "  ip_baseline = pq.read_pandas(\n",
    "    f\"data/{params['input_data']}/ip.parquet\",\n",
    "    [\"rn\", \"imd04_decile\", \"ethnos\", \"admidate\", \"epitype\", \"dismeth\"]\n",
    "  ).to_pandas()\n",
    "\n",
    "  ip_fixed = ip_baseline.merge(ip_rows, on=\"rn\")\n",
    "  \n",
    "  return(ip_fixed, op_fixed)\n",
    "\n",
    "ip, op = split_ip_op_data()\n",
    "\n",
    "ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load the change factors in like so. Note, the order of the rows is semi-important within each model_run:\n",
    "# the \"baseline\" change_factor row must always come first. The other rows are then in the order that change factor\n",
    "# was run within the model engine, but strictly do not need to be shown in that order.\n",
    "change_factors = (ds.dataset(\n",
    "    f\"{results_path}/change_factors/\",\n",
    "    format = \"csv\",\n",
    "    partitioning=\"hive\"\n",
    "  )\n",
    "  .to_table()\n",
    "  .to_pandas()\n",
    ")\n",
    "change_factors"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6517ca32f8fc9ce26110ffab86f836252d161f988376f7559d29b21e62e92b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nhp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
